how do i set up llama.cpp to run locally?

What is the best model to use? codestral-25.01
- how important is token length? what is the expected token length of a program? 1000 lines of code, 1000 words max
    - maybe even longer if we want to add in context pairs. 
how hard is it to get llama.cpp running on a server, and how much would it cost?

where are we injecting the prompt again? do some prompt engineering to figure out the most useful thing to prepend to the user input 

figure out how to cache and group inputs. many people are going to want to see the same things, and we should be able to compose videos about these better if we are able to cache the subcomponents (both performance and determinism wise. bobviously we dont get determinism entirely but its reducing the degrees of freedom for a video on the fast fourier transform when we already have a great visualization on the regular fourier transform x)