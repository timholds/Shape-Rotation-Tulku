how do i set up llama.cpp to run locally?

What is the best model to use? codestral-25.01
- how important is token length? what is the expected token length of a program? 1000 lines of code, 1000 words max
    - maybe even longer if we want to add in context pairs. 
how hard is it to get llama.cpp running on a server, and how much would it cost?

where are we injecting the prompt again? do some prompt engineering to figure out the most useful thing to prepend to the user input 

